{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize    \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "with open(\"clean1.data\") as f:\n",
    "    lines1 = f.readlines()\n",
    "    \n",
    "with open(\"clean2.data\")  as f:\n",
    "    lines2 = f.readlines()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(\"MUSK-jf78,MUSK-jf67,MUSK-jf59,MUSK-jf58,MUSK-jf47,MUSK-jf46,MUSK-jf17,MUSK-j51,MUSK-j33,MUSK-f205,MUSK-f184,MUSK-f159,MUSK-f158,MUSK-f152,MUSK-344,MUSK-333,MUSK-331,MUSK-330,MUSK-323,MUSK-322,MUSK-321,MUSK-316,MUSK-315,MUSK-314,MUSK-311,MUSK-301,MUSK-293,MUSK-292,MUSK-285,MUSK-284,MUSK-273,MUSK-272,MUSK-256,MUSK-254,MUSK-246,MUSK-240,MUSK-238,MUSK-236,MUSK-228,MUSK-227,MUSK-224,MUSK-219,MUSK-213,MUSK-212,MUSK-211,MUSK-190,MUSK-188,NON-MUSK-jp13,NON-MUSK-jp10,NON-MUSK-j97,NON-MUSK-j96,NON-MUSK-j93,NON-MUSK-j90,NON-MUSK-j84,NON-MUSK-j83,NON-MUSK-j81,NON-MUSK-j148,NON-MUSK-j147,NON-MUSK-j146,NON-MUSK-j130,NON-MUSK-j129,NON-MUSK-j100,NON-MUSK-f209,NON-MUSK-f164,NON-MUSK-f161,NON-MUSK-f150,NON-MUSK-334,NON-MUSK-327,NON-MUSK-320,NON-MUSK-319,NON-MUSK-318,NON-MUSK-309,NON-MUSK-308,NON-MUSK-305,NON-MUSK-297,NON-MUSK-296,NON-MUSK-295,NON-MUSK-290,NON-MUSK-289,NON-MUSK-288,NON-MUSK-286,NON-MUSK-271,NON-MUSK-257,NON-MUSK-253,NON-MUSK-249,NON-MUSK-247,NON-MUSK-232,NON-MUSK-226,NON-MUSK-220,NON-MUSK-208,NON-MUSK-200,NON-MUSK-199,NON-MUSK-jp13,NON-MUSK-jp10,NON-MUSK-jf79,NON-MUSK-jf18,NON-MUSK-j97,NON-MUSK-j96,NON-MUSK-j90,NON-MUSK-j84,NON-MUSK-j83,NON-MUSK-j81,NON-MUSK-j148,NON-MUSK-j147,NON-MUSK-j146,NON-MUSK-j130,NON-MUSK-j129,NON-MUSK-j100,NON-MUSK-f209,NON-MUSK-f164,NON-MUSK-f161,NON-MUSK-f150,NON-MUSK-f146,NON-MUSK-362,NON-MUSK-361,NON-MUSK-360,NON-MUSK-358,NON-MUSK-338,NON-MUSK-334,NON-MUSK-332,NON-MUSK-328,NON-MUSK-327,NON-MUSK-326,NON-MUSK-320,NON-MUSK-319,NON-MUSK-318,NON-MUSK-309,NON-MUSK-308,NON-MUSK-305,NON-MUSK-297,NON-MUSK-296,NON-MUSK-295,NON-MUSK-290,NON-MUSK-289,NON-MUSK-288,NON-MUSK-286,NON-MUSK-271,NON-MUSK-270,NON-MUSK-253,NON-MUSK-252,NON-MUSK-251,NON-MUSK-249,NON-MUSK-244,NON-MUSK-233,NON-MUSK-232,NON-MUSK-226,NON-MUSK-220,NON-MUSK-216,NON-MUSK-210,NON-MUSK-208,NON-MUSK-207,NON-MUSK-200,NON-MUSK-199,NON-MUSK-197,NON-MUSK-192,MUSK-jf78,MUSK-jf67,MUSK-jf66,MUSK-jf59,MUSK-jf58,MUSK-jf47,MUSK-jf46,MUSK-jf17,MUSK-jf15,MUSK-j51,MUSK-j33,MUSK-f158,MUSK-f152,MUSK-344,MUSK-333,MUSK-331,MUSK-330,MUSK-323,MUSK-322,MUSK-321,MUSK-314,MUSK-306,MUSK-300,MUSK-294,MUSK-287,MUSK-284,MUSK-273,MUSK-256,MUSK-240,MUSK-238,MUSK-228,MUSK-224,MUSK-219,MUSK-217,MUSK-215,MUSK-214,MUSK-213,MUSK-212,MUSK-211\".split(\",\"))\n",
    "# Use only col 0 + cols 2 (inclusive) -> 169 (exclusive)\n",
    "data =np.loadtxt('clean1.data', dtype=int, delimiter=',',usecols=[0] + list(range(2,169)), \\\n",
    "                 converters={0: lambda s: le.transform([str(s)])[0],\\\n",
    "                             168: lambda s: int(float(s))}, encoding='utf-8')\n",
    "data =np.concatenate((data, np.loadtxt('clean2.data', dtype=int, \\\n",
    "                                       delimiter=',',usecols=[0] + list(range(2,169)), \\\n",
    "                                       converters={0: lambda s: le.transform([str(s)])[0],\\\n",
    "                                                   168: lambda s: int(float(s))}, encoding='utf-8')\n",
    "), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7074, 168)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = np.expand_dims(data, )2 # After preprocessing in loadtxt, col 0 bag name, 2-168 features, 169 label\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7074, 168)\n"
     ]
    }
   ],
   "source": [
    "molecule_bags = np.expand_dims(data[:,0], 1)\n",
    "molecule_labels = np.expand_dims(data[:,-1], 1)\n",
    "normed_data = np.concatenate((molecule_bags,normalize(data[:,1:-1], norm='l2', axis=0)), axis=1) \n",
    "normed_data = np.concatenate((normed_data, molecule_labels), axis=1)\n",
    "print(normed_data.shape)\n",
    "# print(normed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:e\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 168, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(normed_data[:5], axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch.shape (?, 168); x.shape (168,)\n",
      "flattened Tensor(\"strided_slice_1:0\", shape=(?,), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected float32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-fd27eb2715c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0minput_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: Mini-batch input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m168\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# x = tf.get_variable(\"x\", [166, 1], initializer=tf.constant_initializer(data[0][:-1]))\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-fd27eb2715c1>\u001b[0m in \u001b[0;36mcompute_cost\u001b[0;34m(minibatch, x, sess)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mflattened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'flattened '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mselected_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Boolean mask over column of bag_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'selected_rows '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mBi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboolean_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    210\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    211\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 212\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    213\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    411\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 328\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected float32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "def prob_x_given_Bi_minus(Bi, x, sess=None): # joint probability of every instance of Bi\n",
    "    tf.assert_type(Bi, tf.float32)\n",
    "    tf.assert_type(x, tf.float32)\n",
    "    \n",
    "#     except AssertionError as e:\n",
    "#         print(e)\n",
    "#         print(\"x.shape \" + str(x.shape))\n",
    "#         print(\"Bi.shape \" + str(Bi.shape))\n",
    "#         break\n",
    "    # First, calculate all exponentials. Bi-x should broadcast x across 0'th axis of Bi\n",
    "    exps = tf.exp(-tf.norm(Bi-x, ord=2, keepdims=True, axis=1)) # Axis is 1 for the norm \\\n",
    "    # because axis=1 is corresponds to the rows\n",
    "    return tf.reduce_prod(tf.map_fn(lambda s:1-s, exps), axis=0, \\\n",
    "                          keepdims=True, name='prob_x_given_Bi_minus')\n",
    "\n",
    "# Remember to normalize inputs...\n",
    "def create_placeholders():\n",
    "    input_mb = tf.placeholder(tf.float32, [None, 168,], name=\"input_minibatch\")\n",
    "    return input_mb\n",
    "\n",
    "def compute_cost(minibatch, x, sess=None): # \n",
    "    joint_prob = tf.constant(0, dtype=tf.float32) # Overall probability we're argmaxing over\n",
    "    try:\n",
    "        assert minibatch.shape == (None, 168,)\n",
    "        assert x.shape == (168,)\n",
    "    except AssertionError as e:\n",
    "        print(\"minibatch.shape \" + str(minibatch.shape) + \"; x.shape \" + str(x.shape))\n",
    "    bag_ids = tf.unique(minibatch[:,0])\n",
    "#     print(\"bag_ids \" + str(sess.run(bag_ids)))\n",
    "    for bag_id in bag_ids:\n",
    "        flattened = minibatch[:,0]\n",
    "        print('flattened ' + str(flattened))\n",
    "        selected_rows = tf.equal(flattened, tf.constant(bag_id, dtype=tf.float32)) #Boolean mask over column of bag_ids\n",
    "        print('selected_rows ' + str(selected_rows))\n",
    "        Bi = tf.boolean_mask(minibatch, selected_rows)\n",
    "        print(\"Bi \" + str(Bi))\n",
    "#         Bi = tf.gather(minibatch, selected_rows)\n",
    "#         print(Bi.shape)\n",
    "        Bi_label = Bi[0][-1]\n",
    "        prob=prob_x_given_Bi_minus(Bi, x)\n",
    "        joint_prob = tf.add(joint_prob, \n",
    "                            tf.log(tf.add(tf.cond(\n",
    "                                tf.equal(Bi_label,tf.constant(0, dtype=tf.float32)), lambda: prob, lambda: 1-prob)\n",
    "                                                  , tf.constant(1*10**(-8), dtype=tf.float32))))\n",
    "    return tf.scalar_mul(tf.constant(-1, dtype=tf.float32), joint_prob)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "input_minibatch = create_placeholders() # TODO: Mini-batch input data.\n",
    "x = tf.get_variable(\"x\", [168,], dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\\\n",
    "# x = tf.get_variable(\"x\", [166, 1], initializer=tf.constant_initializer(data[0][:-1]))\\\n",
    "cost = compute_cost(input_minibatch, x)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "# with tf.Session() as sess:\n",
    "#     print(\"Dtype \" + str(inp.dtype))\n",
    "#     assert inp.dtype ==tf.int32\n",
    "#     assert inp.shape ==(None, 168, 1)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "sess.run(init)\n",
    "\n",
    "#     print(\"Compute cost \" + str(sess.run(compute_cost(normed_data[:,-1], \\\n",
    "#                                 tf.cast(np.expand_dims(normed_data[0][:-1], 1), tf.float32), sess))))\n",
    "#     print(sess.run(x))\n",
    "for i in range(10000):\n",
    "#     inp=tf.expand_dims(normed_data[:10], axis=2)\n",
    "    print(\"IN FORLOOP\")\n",
    "    intermediate_cost = sess.run([optimizer, cost], feed_dict={x:normed_data[:10]})\n",
    "print(intermediate_cost)\n",
    "print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
