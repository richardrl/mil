{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize    \n",
    "from sklearn.preprocessing import LabelEncoder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"clean1.data\") as f:\n",
    "    lines1 = f.readlines()\n",
    "    \n",
    "with open(\"clean2.data\")  as f:\n",
    "    lines2 = f.readlines()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(\"MUSK-jf78,MUSK-jf67,MUSK-jf59,MUSK-jf58,MUSK-jf47,MUSK-jf46,MUSK-jf17,MUSK-j51,MUSK-j33,MUSK-f205,MUSK-f184,MUSK-f159,MUSK-f158,MUSK-f152,MUSK-344,MUSK-333,MUSK-331,MUSK-330,MUSK-323,MUSK-322,MUSK-321,MUSK-316,MUSK-315,MUSK-314,MUSK-311,MUSK-301,MUSK-293,MUSK-292,MUSK-285,MUSK-284,MUSK-273,MUSK-272,MUSK-256,MUSK-254,MUSK-246,MUSK-240,MUSK-238,MUSK-236,MUSK-228,MUSK-227,MUSK-224,MUSK-219,MUSK-213,MUSK-212,MUSK-211,MUSK-190,MUSK-188,NON-MUSK-jp13,NON-MUSK-jp10,NON-MUSK-j97,NON-MUSK-j96,NON-MUSK-j93,NON-MUSK-j90,NON-MUSK-j84,NON-MUSK-j83,NON-MUSK-j81,NON-MUSK-j148,NON-MUSK-j147,NON-MUSK-j146,NON-MUSK-j130,NON-MUSK-j129,NON-MUSK-j100,NON-MUSK-f209,NON-MUSK-f164,NON-MUSK-f161,NON-MUSK-f150,NON-MUSK-334,NON-MUSK-327,NON-MUSK-320,NON-MUSK-319,NON-MUSK-318,NON-MUSK-309,NON-MUSK-308,NON-MUSK-305,NON-MUSK-297,NON-MUSK-296,NON-MUSK-295,NON-MUSK-290,NON-MUSK-289,NON-MUSK-288,NON-MUSK-286,NON-MUSK-271,NON-MUSK-257,NON-MUSK-253,NON-MUSK-249,NON-MUSK-247,NON-MUSK-232,NON-MUSK-226,NON-MUSK-220,NON-MUSK-208,NON-MUSK-200,NON-MUSK-199,NON-MUSK-jp13,NON-MUSK-jp10,NON-MUSK-jf79,NON-MUSK-jf18,NON-MUSK-j97,NON-MUSK-j96,NON-MUSK-j90,NON-MUSK-j84,NON-MUSK-j83,NON-MUSK-j81,NON-MUSK-j148,NON-MUSK-j147,NON-MUSK-j146,NON-MUSK-j130,NON-MUSK-j129,NON-MUSK-j100,NON-MUSK-f209,NON-MUSK-f164,NON-MUSK-f161,NON-MUSK-f150,NON-MUSK-f146,NON-MUSK-362,NON-MUSK-361,NON-MUSK-360,NON-MUSK-358,NON-MUSK-338,NON-MUSK-334,NON-MUSK-332,NON-MUSK-328,NON-MUSK-327,NON-MUSK-326,NON-MUSK-320,NON-MUSK-319,NON-MUSK-318,NON-MUSK-309,NON-MUSK-308,NON-MUSK-305,NON-MUSK-297,NON-MUSK-296,NON-MUSK-295,NON-MUSK-290,NON-MUSK-289,NON-MUSK-288,NON-MUSK-286,NON-MUSK-271,NON-MUSK-270,NON-MUSK-253,NON-MUSK-252,NON-MUSK-251,NON-MUSK-249,NON-MUSK-244,NON-MUSK-233,NON-MUSK-232,NON-MUSK-226,NON-MUSK-220,NON-MUSK-216,NON-MUSK-210,NON-MUSK-208,NON-MUSK-207,NON-MUSK-200,NON-MUSK-199,NON-MUSK-197,NON-MUSK-192,MUSK-jf78,MUSK-jf67,MUSK-jf66,MUSK-jf59,MUSK-jf58,MUSK-jf47,MUSK-jf46,MUSK-jf17,MUSK-jf15,MUSK-j51,MUSK-j33,MUSK-f158,MUSK-f152,MUSK-344,MUSK-333,MUSK-331,MUSK-330,MUSK-323,MUSK-322,MUSK-321,MUSK-314,MUSK-306,MUSK-300,MUSK-294,MUSK-287,MUSK-284,MUSK-273,MUSK-256,MUSK-240,MUSK-238,MUSK-228,MUSK-224,MUSK-219,MUSK-217,MUSK-215,MUSK-214,MUSK-213,MUSK-212,MUSK-211\".split(\",\"))\n",
    "# Use only col 0 + cols 2 (inclusive) -> 169 (exclusive)\n",
    "data =np.loadtxt('clean1.data', dtype=int, delimiter=',',usecols=[0] + list(range(2,169)), \\\n",
    "                 converters={0: lambda s: le.transform([str(s)])[0],\\\n",
    "                             168: lambda s: int(float(s))}, encoding='utf-8')\n",
    "data =np.concatenate((data, np.loadtxt('clean2.data', dtype=int, \\\n",
    "                                       delimiter=',',usecols=[0] + list(range(2,169)), \\\n",
    "                                       converters={0: lambda s: le.transform([str(s)])[0],\\\n",
    "                                                   168: lambda s: int(float(s))}, encoding='utf-8')\n",
    "), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7074, 168)\n"
     ]
    }
   ],
   "source": [
    "molecule_bags = np.expand_dims(data[:,0], 1)\n",
    "molecule_labels = np.expand_dims(data[:,-1], 1)\n",
    "normed_data = np.concatenate((molecule_bags,normalize(data[:,1:-1], norm='l2', axis=0)), axis=1) \n",
    "normed_data = np.concatenate((normed_data, molecule_labels), axis=1)\n",
    "print(normed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tfe.Variable(np.random.uniform(size=[166,]), name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_x_given_Bi_minus(Bi, x):\n",
    "    exps = tf.exp(-tf.norm(Bi-x, ord=2, keepdims=True, axis=1)) # Axis is 1 for the norm \\\n",
    "    return tf.reduce_prod(tf.map_fn(lambda s:1-s, exps), axis=0, \\\n",
    "                          keepdims=True, name='prob_x_given_Bi_minus')\n",
    "\n",
    "def get_bags(minibatch, bag_idx, x):\n",
    "    bag_id_labels = minibatch[:,0]\n",
    "    rows = tf.gather(minibatch,tf.where(tf.equal(bag_id_labels, bag_id)))\n",
    "    return prob_x_given_Bi_minus(rows, x)\n",
    "\n",
    "def compute_cost(minibatch, x):\n",
    "    joint_prob = tf.constant(0, dtype=tf.float64) # Overall probability we're argmaxing over\n",
    "    bag_ids = tf.unique(minibatch[:,0])[0]\n",
    "    for i in range(bag_ids.shape[0]):\n",
    "        bag_id = bag_ids[i]\n",
    "        bag_id_labels = minibatch[:,0]\n",
    "        where = tf.where(tf.equal(bag_id_labels, bag_id))\n",
    "        rows = tf.gather(minibatch,tf.reshape(where, [where.shape[0],]))\n",
    "        Bi, Bi_label = rows[:,1:-1], rows[0,-1]\n",
    "        prob=prob_x_given_Bi_minus(Bi, x)[0][0]\n",
    "        cons = tf.constant(1*10**-8, dtype=tf.float64)\n",
    "        inner_sum = tf.add(prob, cons)\n",
    "        log_result = tf.log(inner_sum)\n",
    "        joint_prob = tf.add(joint_prob, log_result)\n",
    "    return -joint_prob\n",
    "\n",
    "# def get_minibatches(dataset):\n",
    "#     \"\"\" Shuffle and return minibatches for dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "# x=tfe.Variable(np.random.uniform(size=[166,]), name='x', dtype=tf.float64)\n",
    "# f_grad = tfe.gradients_function(lambda s: compute_cost(tf.constant(normed_data[:10], dtype=tf.float64), s), params=['s'])\n",
    "# print(\"F grad \" + str(f_grad(x) + '\\n'))\n",
    "# print(\"\\nGradient \" + str(f_grad(x)))\n",
    "# f_grad = tfe.gradients_function(compute_cost, params=['x'])\n",
    "\n",
    "x=tfe.Variable(np.random.uniform(size=[166,]), name='x', dtype=tf.float64)\n",
    "# print(\"x before\" + str(x))\n",
    "print(\"cost before\")\n",
    "print(str(compute_cost(tf.constant(normed_data[:1000], dtype=tf.float64), x)) + '\\n')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "#     minibatch_q = tf.RandomShuffleQueue(normed_data.shape[0], 0, tf.float64, shapes=[168,])\n",
    "#     minibatch_q.enqueue_many(normed_data[:312])\n",
    "#     for i in range(3):\n",
    "#         minibatch = minibatch_q.dequeue_many(128)\n",
    "    shuffled = tf.random_shuffle(normed_data)\n",
    "    print(\"epoch \" + str(epoch))\n",
    "    for i in range(len(shuffled)):\n",
    "        mb_start = i\n",
    "        mb_end = i + 128 if (i+128) <= len(normed_data) else len(normed_data)\n",
    "        optimizer.minimize(lambda: compute_cost(tf.constant(shuffled[mb_start:mb_end], dtype=tf.float64), x), var_list=[x])\n",
    "        print(\"cost \", end=\"\")\n",
    "        print(compute_cost(tf.constant(normed_data, dtype=tf.float64), x))\n",
    "print(\"cost after\")\n",
    "print(compute_cost(tf.constant(normed_data[:1000], dtype=tf.float64), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def compute_cost(x):\n",
    "#     return x*5\n",
    "# def train(model, optimizer, dataset, step_counter, log_interval=None):\n",
    "#     \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
    "\n",
    "#     start = time.time()\n",
    "#     for (batch, (images, labels)) in enumerate(dataset):\n",
    "#     with tf.contrib.summary.record_summaries_every_n_global_steps(\n",
    "#         10, global_step=step_counter):\n",
    "#       # Record the operations used to compute the loss given the input,\n",
    "#       # so that the gradient of the loss with respect to the variables\n",
    "#       # can be computed.\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             logits = model(images, training=True)\n",
    "#             loss_value = loss(logits, labels)\n",
    "#             tf.contrib.summary.scalar('loss', loss_value)\n",
    "#             tf.contrib.summary.scalar('accuracy', compute_accuracy(logits, labels))\n",
    "#             grads = tape.gradient(loss_value, model.variables)\n",
    "#             optimizer.apply_gradients(\n",
    "#               zip(grads, model.variables), global_step=step_counter)\n",
    "#             if log_interval and batch % log_interval == 0:\n",
    "#                 rate = log_interval / (time.time() - start)\n",
    "#                 print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (batch, loss_value, rate))\n",
    "#                 start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
