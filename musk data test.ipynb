{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize    \n",
    "from sklearn.preprocessing import LabelEncoder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"clean1.data\") as f:\n",
    "    lines1 = f.readlines()\n",
    "    \n",
    "with open(\"clean2.data\")  as f:\n",
    "    lines2 = f.readlines()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(\"MUSK-jf78,MUSK-jf67,MUSK-jf59,MUSK-jf58,MUSK-jf47,MUSK-jf46,MUSK-jf17,MUSK-j51,MUSK-j33,MUSK-f205,MUSK-f184,MUSK-f159,MUSK-f158,MUSK-f152,MUSK-344,MUSK-333,MUSK-331,MUSK-330,MUSK-323,MUSK-322,MUSK-321,MUSK-316,MUSK-315,MUSK-314,MUSK-311,MUSK-301,MUSK-293,MUSK-292,MUSK-285,MUSK-284,MUSK-273,MUSK-272,MUSK-256,MUSK-254,MUSK-246,MUSK-240,MUSK-238,MUSK-236,MUSK-228,MUSK-227,MUSK-224,MUSK-219,MUSK-213,MUSK-212,MUSK-211,MUSK-190,MUSK-188,NON-MUSK-jp13,NON-MUSK-jp10,NON-MUSK-j97,NON-MUSK-j96,NON-MUSK-j93,NON-MUSK-j90,NON-MUSK-j84,NON-MUSK-j83,NON-MUSK-j81,NON-MUSK-j148,NON-MUSK-j147,NON-MUSK-j146,NON-MUSK-j130,NON-MUSK-j129,NON-MUSK-j100,NON-MUSK-f209,NON-MUSK-f164,NON-MUSK-f161,NON-MUSK-f150,NON-MUSK-334,NON-MUSK-327,NON-MUSK-320,NON-MUSK-319,NON-MUSK-318,NON-MUSK-309,NON-MUSK-308,NON-MUSK-305,NON-MUSK-297,NON-MUSK-296,NON-MUSK-295,NON-MUSK-290,NON-MUSK-289,NON-MUSK-288,NON-MUSK-286,NON-MUSK-271,NON-MUSK-257,NON-MUSK-253,NON-MUSK-249,NON-MUSK-247,NON-MUSK-232,NON-MUSK-226,NON-MUSK-220,NON-MUSK-208,NON-MUSK-200,NON-MUSK-199,NON-MUSK-jp13,NON-MUSK-jp10,NON-MUSK-jf79,NON-MUSK-jf18,NON-MUSK-j97,NON-MUSK-j96,NON-MUSK-j90,NON-MUSK-j84,NON-MUSK-j83,NON-MUSK-j81,NON-MUSK-j148,NON-MUSK-j147,NON-MUSK-j146,NON-MUSK-j130,NON-MUSK-j129,NON-MUSK-j100,NON-MUSK-f209,NON-MUSK-f164,NON-MUSK-f161,NON-MUSK-f150,NON-MUSK-f146,NON-MUSK-362,NON-MUSK-361,NON-MUSK-360,NON-MUSK-358,NON-MUSK-338,NON-MUSK-334,NON-MUSK-332,NON-MUSK-328,NON-MUSK-327,NON-MUSK-326,NON-MUSK-320,NON-MUSK-319,NON-MUSK-318,NON-MUSK-309,NON-MUSK-308,NON-MUSK-305,NON-MUSK-297,NON-MUSK-296,NON-MUSK-295,NON-MUSK-290,NON-MUSK-289,NON-MUSK-288,NON-MUSK-286,NON-MUSK-271,NON-MUSK-270,NON-MUSK-253,NON-MUSK-252,NON-MUSK-251,NON-MUSK-249,NON-MUSK-244,NON-MUSK-233,NON-MUSK-232,NON-MUSK-226,NON-MUSK-220,NON-MUSK-216,NON-MUSK-210,NON-MUSK-208,NON-MUSK-207,NON-MUSK-200,NON-MUSK-199,NON-MUSK-197,NON-MUSK-192,MUSK-jf78,MUSK-jf67,MUSK-jf66,MUSK-jf59,MUSK-jf58,MUSK-jf47,MUSK-jf46,MUSK-jf17,MUSK-jf15,MUSK-j51,MUSK-j33,MUSK-f158,MUSK-f152,MUSK-344,MUSK-333,MUSK-331,MUSK-330,MUSK-323,MUSK-322,MUSK-321,MUSK-314,MUSK-306,MUSK-300,MUSK-294,MUSK-287,MUSK-284,MUSK-273,MUSK-256,MUSK-240,MUSK-238,MUSK-228,MUSK-224,MUSK-219,MUSK-217,MUSK-215,MUSK-214,MUSK-213,MUSK-212,MUSK-211\".split(\",\"))\n",
    "# Use only col 0 + cols 2 (inclusive) -> 169 (exclusive)\n",
    "data =np.loadtxt('clean1.data', dtype=int, delimiter=',',usecols=[0] + list(range(2,169)), \\\n",
    "                 converters={0: lambda s: le.transform([str(s)])[0],\\\n",
    "                             168: lambda s: int(float(s))}, encoding='utf-8')\n",
    "data =np.concatenate((data, np.loadtxt('clean2.data', dtype=int, \\\n",
    "                                       delimiter=',',usecols=[0] + list(range(2,169)), \\\n",
    "                                       converters={0: lambda s: le.transform([str(s)])[0],\\\n",
    "                                                   168: lambda s: int(float(s))}, encoding='utf-8')\n",
    "), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7074, 168)\n"
     ]
    }
   ],
   "source": [
    "molecule_bags = np.expand_dims(data[:,0], 1)\n",
    "molecule_labels = np.expand_dims(data[:,-1], 1)\n",
    "normed_data = np.concatenate((molecule_bags,normalize(data[:,1:-1], norm='l2', axis=0)), axis=1) \n",
    "normed_data = np.concatenate((normed_data, molecule_labels), axis=1)\n",
    "print(normed_data.shape)\n",
    "# print(normed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tfe.Variable(np.random.uniform(size=[166,]), name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag ids\n",
      "[0. 1. 2.]\n",
      "np where\n",
      "(array([0, 1, 2, 3]),)\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 2. 2.]\n",
      "rows (4, 168)\n",
      "Bi\n",
      "(4, 166)\n",
      "np where\n",
      "(array([4, 5, 6, 7]),)\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 2. 2.]\n",
      "rows (4, 168)\n",
      "Bi\n",
      "(4, 166)\n",
      "np where\n",
      "(array([8, 9]),)\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 2. 2.]\n",
      "rows (2, 168)\n",
      "Bi\n",
      "(2, 166)\n",
      "[None]\n",
      "F grad 1\n"
     ]
    }
   ],
   "source": [
    "def prob_x_given_Bi_minus(Bi, x):\n",
    "    exps = tf.exp(-tf.norm(Bi-x, ord=2, keepdims=True, axis=1)) # Axis is 1 for the norm \\\n",
    "    return tf.reduce_prod(tf.map_fn(lambda s:1-s, exps), axis=0, \\\n",
    "                          keepdims=True, name='prob_x_given_Bi_minus')\n",
    "\n",
    "def compute_cost(minibatch, x):\n",
    "    joint_prob = tf.constant(0) # Overall probability we're argmaxing over\n",
    "    bag_ids = np.unique(minibatch[:,0])\n",
    "    print(\"Bag ids\")\n",
    "    print(bag_ids)\n",
    "    for bag_id in bag_ids:\n",
    "        bag_id_labels = minibatch[:,0]\n",
    "        print(\"np where\")\n",
    "        print(np.where(bag_id_labels == bag_id))\n",
    "        print(minibatch[:,0])\n",
    "        rows = minibatch[np.where(bag_id_labels == bag_id)]\n",
    "        print(\"rows \" + str(rows.shape))\n",
    "        Bi, Bi_label = rows[:,1:-1], rows[0,-1]\n",
    "        print(\"Bi\")\n",
    "        print(Bi.shape)\n",
    "        prob=prob_x_given_Bi_minus(Bi, x)\n",
    "        joint_prob += np.log(prob + 1*10**-8)\n",
    "    return -joint_prob\n",
    "\n",
    "# def compute_cost(x):\n",
    "#     return x*5\n",
    "# def train(model, optimizer, dataset, step_counter, log_interval=None):\n",
    "#     \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
    "\n",
    "#     start = time.time()\n",
    "#     for (batch, (images, labels)) in enumerate(dataset):\n",
    "#     with tf.contrib.summary.record_summaries_every_n_global_steps(\n",
    "#         10, global_step=step_counter):\n",
    "#       # Record the operations used to compute the loss given the input,\n",
    "#       # so that the gradient of the loss with respect to the variables\n",
    "#       # can be computed.\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             logits = model(images, training=True)\n",
    "#             loss_value = loss(logits, labels)\n",
    "#             tf.contrib.summary.scalar('loss', loss_value)\n",
    "#             tf.contrib.summary.scalar('accuracy', compute_accuracy(logits, labels))\n",
    "#             grads = tape.gradient(loss_value, model.variables)\n",
    "#             optimizer.apply_gradients(\n",
    "#               zip(grads, model.variables), global_step=step_counter)\n",
    "#             if log_interval and batch % log_interval == 0:\n",
    "#                 rate = log_interval / (time.time() - start)\n",
    "#                 print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (batch, loss_value, rate))\n",
    "#                 start = time.time()\n",
    "\n",
    "f_grad = tfe.gradients_function(lambda x: compute_cost(normed_data[:10], x), params=['x'])\n",
    "print(f_grad(x))\n",
    "# print(\"F grad \" + str(f_grad(x) + '\\n'))\n",
    "print(\"F grad 1\")\n",
    "# f_grad = tfe.gradients_function(compute_cost, params=['x'])\n",
    "\n",
    "# print(type(compute_cost(normed_data[:10], x)))\n",
    "\n",
    "# def square_f(W):\n",
    "#   # Return a tensor with elements squared\n",
    "#   return tf.square(W)\n",
    "\n",
    "# f_grad = tfe.gradients_function(square_f, params=['W'])\n",
    "# print(f_grad(tf.constant(0.3))) \n",
    "\n",
    "# print(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "# optimizer = tf.train.AdamOptimizer()\n",
    "# optimizer.minimize(lambda: compute_cost(tf.constant(normed_data[:10]), x), var_list=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
